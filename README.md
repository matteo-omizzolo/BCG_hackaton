# Customer Churn Prediction Challenge

> Endâ€‘toâ€‘end analytics and modelling pipeline for predicting churn, understanding its drivers and estimating the â‚¬â€‘impact of retention strategies.

&#x20;

## Problem Statement

Telecom providers lose a significant share of revenue when dissatisfied clients decide to leave (â€œchurnâ€).
This repository contains our solution to **BCGâ€™s Customerâ€‘Churn Hackathon**: given two raw Excel files with 7â€¯043 customers and their complaint history, build a model that flags customers at risk of churning and quantify the financial upside of saving them.

* `clients.xlsx` â€“ demographic & subscription attributes
* `complaints.xlsx` â€“ freeâ€‘text complaints, one row per ticket

## Repository layout

```
â”œâ”€â”€ code/
â”‚Â Â  â”œâ”€â”€ EDA.ipynb          # data exploration & cleansing
â”‚Â Â  â”œâ”€â”€ Logistic.ipynb     # baseline model + businessâ€‘friendly explainability
â”‚Â Â  â”œâ”€â”€ XGBoost.ipynb      # gradientâ€‘boosting, classâ€‘imbalance pipeline & â‚¬ impact
â”‚Â Â  â””â”€â”€ BCG_text.ipynb     # NLP: topic / emotion mining on complaint text
â”œâ”€â”€ data                   # data used for the challenge
â”œâ”€â”€ BCG_presentation.pptx  # presentation of results
â””â”€â”€ README.md              # you are here ğŸš€
```

## Quick start

```bash
# 1 â€‘ clone
git clone https://github.com/<yourâ€‘handle>/<repoâ€‘name>.git
cd <repoâ€‘name>

# 2 â€‘ create environment
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt        # ~5â€¯min

# 3 â€‘ reproduce results
jupyter lab
# run notebooks in the order shown above â€“ every notebook is seeded for reproducibility
```

> **Tip:** large plots are saved to `figures/` automatically; set `SAVE_FIG=False` in `src/config.py` if you prefer inline only.

## Results

| Model                  | Accuracy  | Recall    | Precision | F1â€‘score | ROCâ€‘AUC   |
| ---------------------- | --------- | --------- | --------- | -------- | --------- |
| Logistic Regression    | **0.746** | 0.821     | 0.513     | 0.632    | 0.846     |
| XGBoost (best 50 iter) | 0.740     | **0.842** | 0.506     | 0.633    | **0.853** |

*The XGBoost pipeline slightly improves AUC and recall â€“ the two metrics the business cares about â€“ while keeping precision on par with the baseline.*

## Business impact

* Retaining the **churneers actually predicted by the model** yields an estimated **â‚¬â€¯560k** additional recurring revenue over the next 37â€¯months of which 41% in the first 12 months (see `XGBoost.ipynb â†’ Cost analysis`).
* Textâ€‘mining highlights *price*, *billing issues* and *internet issues* as the complaint topics most associated with churn.
  Marketing can prioritise these pain points in retention campaigns.

## How it works

1. **EDA & cleaning**
   Missing `TotalCharges` values are imputed from `MonthlyCharges Ã— tenure`; categorical features are oneâ€‘hot encoded.
2. **Modelling**

   * **Baseline:** logistic regression with scaled numerics, *SelectKBest* and classâ€‘weight tuning.
   * **Advanced:** XGBoost inside an imbalancedâ€‘learn pipeline + grid hyperâ€‘parameter search with cross-validation and log-loss function.
3. **Explainability**

   * Feature importance (SHAP & logâ€‘odds)
   * Partial Dependence & demographic slice plots
4. **Text analytics**
   BERT âœ PCA âœ Kâ€‘Means to group complaints, LDA topics & NRC emotion lexicon to surface dominant feelings.

## Contributing

Pull requests are welcome! Please open an issue first to discuss major changes.

## Author

Giacomo Negri â€“ [@GiacomoNegri](https://github.com/GiacomoNegri)
Jacopo D'Angelo â€“ [@jacopomattia03](https://github.com/jacopomattia03)
Matteo Omizzolo â€“ [@matteo-omizzolo](https://github.com/matteo-omizzolo)
Ioannis Thomopoulos â€“ [@ioannis0909](https://github.com/ioannis0909)
- feel free to reach out for questions or collaboration.
